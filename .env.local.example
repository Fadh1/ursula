# AI Model Configuration
# Copy this file to .env.local and fill in your API keys and endpoints

# OpenAI Configuration
VITE_OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration
VITE_ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google AI Configuration
VITE_GOOGLE_API_KEY=your_google_ai_api_key_here

# Local Model Endpoints
# Ollama (default: http://localhost:11434)
VITE_OLLAMA_BASE_URL=http://localhost:11434

# LM Studio (default: http://localhost:1234)
VITE_LM_STUDIO_BASE_URL=http://localhost:1234

# Notes:
# - Only configure the providers you want to use
# - API keys are required for cloud providers (OpenAI, Anthropic, Google)
# - Local endpoints (Ollama, LM Studio) don't require API keys
# - Make sure local services are running before using them
# - Restart the application after changing environment variables